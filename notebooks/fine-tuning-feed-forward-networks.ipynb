{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import optimizers, backend as K\n",
    "from keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sacred import Experiment, utils as sacred_utils\n",
    "\n",
    "from connoisseur import get_preprocess_fn\n",
    "from connoisseur.datasets import load_pickle_data\n",
    "from connoisseur.datasets.painter_by_numbers import load_multiple_outputs\n",
    "from connoisseur.utils.image import MultipleOutputsDirectorySequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gram_matrix = False\n",
    "pooling = 'avg'\n",
    "outputs_meta = [\n",
    "    {'n': 'artist', 'u': 1584, 'a': 'softmax',\n",
    "     'l': 'categorical_crossentropy',\n",
    "     'm': ['categorical_accuracy', 'top_k_categorical_accuracy'],\n",
    "     'w': .5},\n",
    "    {'n': 'style', 'u': 135, 'a': 'softmax',\n",
    "     'l': 'categorical_crossentropy',\n",
    "     'm': ['categorical_accuracy', 'top_k_categorical_accuracy'],\n",
    "     'w': .2},\n",
    "    {'n': 'genre', 'u': 42, 'a': 'softmax',\n",
    "     'l': 'categorical_crossentropy',\n",
    "     'm': ['categorical_accuracy', 'top_k_categorical_accuracy'],\n",
    "     'w': .2},\n",
    "    {'n': 'date', 'u': 1, 'a': 'linear',\n",
    "     'l': 'mse', 'm': 'mae', 'w': .1}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/work/datasets/patches/299-balanced-inceptionrnv2-299\"\n",
    "data = load_pickle_data(data_dir, phases=['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown year ific\n",
      "unknown year rain\n",
      "unknown year rver\n"
     ]
    }
   ],
   "source": [
    "train_info = '/datasets/pbn/train_info.csv'\n",
    "outputs, name_map = load_multiple_outputs(train_info, outputs_meta, encode='onehot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle = True\n",
    "valid_shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers available: dict_keys(['global_average_pooling2d_1'])\n"
     ]
    }
   ],
   "source": [
    "print('layers available:', data['train'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-train, x-valid shape: (3192922, 1536) (1577778, 1536)\n"
     ]
    }
   ],
   "source": [
    "layer_name = 'global_average_pooling2d_1'\n",
    "\n",
    "(x_train, _, names_train), (x_valid, _, names_valid) = data['train'], data['valid']\n",
    "x_train, x_valid = (x[layer_name] for x in (x_train, x_valid))\n",
    "print('x-train, x-valid shape:', x_train.shape, x_valid.shape)\n",
    "\n",
    "p = np.arange(len(x_train))\n",
    "np.random.shuffle(p)\n",
    "x_train = x_train[p]\n",
    "names_train = names_train[p]\n",
    "\n",
    "p = np.arange(len(x_valid))\n",
    "np.random.shuffle(p)\n",
    "x_valid = x_valid[p]\n",
    "names_valid = names_valid[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "for phase, names in zip(('train', 'valid'),\n",
    "                        (names_train, names_valid)):\n",
    "    names = ['-'.join(os.path.basename(n).split('-')[:-1]) for n in names]\n",
    "    indices = [name_map[n] for n in names]\n",
    "    ys += [{o: v[indices] for o, v in outputs.items()}]\n",
    "\n",
    "y_train, y_valid = ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for y in (y_train, y_valid):\n",
    "    f = plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    groups = 'artist style genre'.split()\n",
    "    \n",
    "    for ix, group in enumerate(groups):\n",
    "        _y = y[group]\n",
    "        l, c = np.unique(np.argmax(_y, axis=1), return_counts=True)\n",
    "        \n",
    "        print('unique values:', len(l))\n",
    "        \n",
    "        ax = f.add_subplot(1, 3, ix + 1)\n",
    "        plt.bar(l, c)\n",
    "        plt.title(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Limb Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = '/gpu:0'\n",
    "weights = 'imagenet'\n",
    "last_base_layer = None\n",
    "use_gram_matrix = False\n",
    "pooling = 'avg'\n",
    "ckpt_file = 'weights.hdf5'\n",
    "shape = [1536]\n",
    "dense_layers=[2048, 2048]\n",
    "\n",
    "device = \"/gpu:0\"\n",
    "\n",
    "opt_params = {'lr': .001}\n",
    "dropout_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model, Input\n",
    "from keras.layers import Flatten, Lambda, Dense, Dropout\n",
    "\n",
    "def build_meta_limb(shape, dropout_p=.5,\n",
    "                    classes=1000, use_gram_matrix=False,\n",
    "                    dense_layers=(),\n",
    "                    include_top=True,\n",
    "                    predictions_activation='softmax',\n",
    "                    predictions_name='predictions', model_name=None):\n",
    "    x = Input(shape=shape)\n",
    "\n",
    "    if use_gram_matrix:\n",
    "        sizes = K.get_variable_shape(x)\n",
    "        k = sizes[-1]\n",
    "        y = Lambda(gram_matrix, arguments=dict(norm_by_channels=False),\n",
    "                   name='gram', output_shape=[k, k])(x)\n",
    "    else:\n",
    "        y = x\n",
    "\n",
    "    if include_top:\n",
    "        if K.ndim(y) > 2:\n",
    "            y = Flatten(name='flatten')(y)\n",
    "\n",
    "        for l_id, n_units in enumerate(dense_layers):\n",
    "            y = Dense(n_units, activation='relu', name='fc%i' % l_id)(y)\n",
    "            y = Dropout(dropout_p)(y)\n",
    "\n",
    "        if not isinstance(classes, (list, tuple)):\n",
    "            classes, predictions_activation, predictions_name = (\n",
    "                [classes], [predictions_activation], [predictions_name])\n",
    "        outputs = []\n",
    "        for u, a, n in zip(classes, predictions_activation, predictions_name):\n",
    "            outputs += [Dense(u, activation=a, name=n)(y)]\n",
    "    else:\n",
    "        outputs = [y]\n",
    "\n",
    "    return Model(inputs=x, outputs=outputs, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1536)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc0 (Dense)                     (None, 2048)         3147776     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           fc0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "fc1 (Dense)                     (None, 2048)         4196352     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           fc1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "artist (Dense)                  (None, 1584)         3245616     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "style (Dense)                   (None, 135)          276615      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "genre (Dense)                   (None, 42)           86058       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "date (Dense)                    (None, 1)            2049        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,954,466\n",
      "Trainable params: 10,954,466\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):\n",
    "    print('building...')\n",
    "    model = build_meta_limb(shape, dropout_p=dropout_p,\n",
    "                            use_gram_matrix=use_gram_matrix,\n",
    "                            include_top=True,\n",
    "                            dense_layers=dense_layers,\n",
    "                            classes=[o['u'] for o in outputs_meta],\n",
    "                            predictions_name=[o['n'] for o in outputs_meta],\n",
    "                            predictions_activation=[o['a'] for o in outputs_meta])\n",
    "    \n",
    "    model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):\n",
    "    model.compile(optimizer=optimizers.Adam(**opt_params),\n",
    "                  loss=dict((o['n'], o['l']) for o in outputs_meta),\n",
    "                  metrics=dict((o['n'], o['m']) for o in outputs_meta),\n",
    "                  loss_weights=dict((o['n'], o['w']) for o in outputs_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "initial_epoch = 0\n",
    "batch_size = 4096\n",
    "\n",
    "steps_per_epoch = 500\n",
    "validation_steps = None\n",
    "\n",
    "workers = 8\n",
    "use_multiprocessing = True\n",
    "\n",
    "early_stop_patience = 100\n",
    "reduce_lr_patience = int(early_stop_patience // 3)\n",
    "\n",
    "class_weight = None\n",
    "ckpt = 'meta-balanced-mo-%s.h5' % datetime.now().date()\n",
    "report_dir = '/work/painter-by-numbers/' + ckpt.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuming_from = None\n",
    "if resuming_from:\n",
    "    print('re-loading weights...')\n",
    "    model.load_weights(resuming_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training from epoch 0...\n",
      "Train on 3192922 samples, validate on 1577778 samples\n",
      "Epoch 1/500\n",
      "Epoch 00001: val_loss improved from inf to 3.39281, saving model to meta-balanced-mo-2018-10-18.h5\n",
      " - 232s - loss: 2.6289 - artist_loss: 3.4762 - style_loss: 2.3677 - genre_loss: 1.6810 - date_loss: 0.8106 - artist_categorical_accuracy: 0.3593 - artist_top_k_categorical_accuracy: 0.5285 - style_categorical_accuracy: 0.3331 - style_top_k_categorical_accuracy: 0.7143 - genre_categorical_accuracy: 0.5084 - genre_top_k_categorical_accuracy: 0.8466 - date_mean_absolute_error: 0.5876 - val_loss: 3.3928 - val_artist_loss: 5.0111 - val_style_loss: 2.4516 - val_genre_loss: 1.6529 - val_date_loss: 0.6635 - val_artist_categorical_accuracy: 0.2165 - val_artist_top_k_categorical_accuracy: 0.3876 - val_style_categorical_accuracy: 0.3323 - val_style_top_k_categorical_accuracy: 0.7207 - val_genre_categorical_accuracy: 0.5220 - val_genre_top_k_categorical_accuracy: 0.8521 - val_date_mean_absolute_error: 0.5341\n",
      "Epoch 2/500\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 213s - loss: 1.5086 - artist_loss: 1.6541 - style_loss: 1.7229 - genre_loss: 1.3632 - date_loss: 0.6430 - artist_categorical_accuracy: 0.6452 - artist_top_k_categorical_accuracy: 0.8030 - style_categorical_accuracy: 0.4884 - style_top_k_categorical_accuracy: 0.8395 - genre_categorical_accuracy: 0.5894 - genre_top_k_categorical_accuracy: 0.9004 - date_mean_absolute_error: 0.5313 - val_loss: 3.5572 - val_artist_loss: 5.3373 - val_style_loss: 2.4771 - val_genre_loss: 1.6461 - val_date_loss: 0.6390 - val_artist_categorical_accuracy: 0.2324 - val_artist_top_k_categorical_accuracy: 0.4004 - val_style_categorical_accuracy: 0.3480 - val_style_top_k_categorical_accuracy: 0.7295 - val_genre_categorical_accuracy: 0.5311 - val_genre_top_k_categorical_accuracy: 0.8537 - val_date_mean_absolute_error: 0.5335\n",
      "Epoch 3/500\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('training from epoch %i...' % initial_epoch)\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(x_valid, y_valid),\n",
    "              initial_epoch=initial_epoch,\n",
    "              verbose=2,\n",
    "              class_weight=class_weight,\n",
    "              callbacks=[\n",
    "                  TerminateOnNaN(),\n",
    "                  EarlyStopping(patience=early_stop_patience),\n",
    "                  ReduceLROnPlateau(min_lr=1e-10, patience=reduce_lr_patience),\n",
    "                  TensorBoard(report_dir, batch_size=batch_size),\n",
    "                  ModelCheckpoint(ckpt,\n",
    "                                  save_best_only=True,\n",
    "                                  verbose=1)\n",
    "              ])\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupted by user')\n",
    "else:\n",
    "    print('done')\n",
    "finally:\n",
    "    print('train history:', model.history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
